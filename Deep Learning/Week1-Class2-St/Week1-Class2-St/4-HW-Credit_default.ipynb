{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Credit Worthiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Banks play a crucial role in market economies. They decide who can get finance and on what terms and can make or break investment decisions. For markets and society to function, individuals and companies need access to credit. \n",
    "\n",
    "Credit scoring algorithms, which make a guess at the probability of default, are the method banks use to determine whether or not a loan should be granted. This competition requires participants to improve on the state of the art in credit scoring, by predicting the probability that somebody will experience financial distress in the next two years.\n",
    "\n",
    "The goal is to build a model that borrowers can use to help make the best financial decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the Dataset and prepare it for the algorithm\n",
    "- Load the dataset, check the first five observations, check the column names, check the summary of the dataset (describe method) to see if there are any outliers or any missing values.\n",
    "\n",
    "- If there are any outliers, cap the dataset with 0.95 quantile:\n",
    "```\n",
    " x=data['columnname'].quantile(0.95)\n",
    " data['columnname']=np.where(data['columnname']>x,x,data['columnname'])\n",
    " ```\n",
    "\n",
    "- Split data into atribute set and target\n",
    "\n",
    "- split into test and train data sets\n",
    "\n",
    "- If you see any need for scaling, scale the data column by dividing by maximum of that column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4myEEoOBtfX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('cs-training.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set up a baseline model \n",
    "\n",
    "- Use one hidden layer with activation 'relu'\n",
    "- What activation should you use for output? Remember that this is a classification algorithm.\n",
    "- Compile the model\n",
    "- Fir the model, pick your epoch numbers and batch size. Use Test set as validation. \n",
    "- Visulaize train and test score as a function of epochs\n",
    "- You can also use the `predict` method of your model and import `metrics` module from ScikitLearn to check accuracy metris in your test data set\n",
    "- If you like you can use scikitlearn API for this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Controlling overfitting\n",
    "\n",
    "- I tried to reduce overfitting myself and coudlnt succeed. Do not be surprised if any of the measures below does not help reducing the overfitting.\n",
    "- Import `l2` fom `keras` and introduce `kernel_regularization` to both layers of your model\n",
    "- Run `0.75` dropout with `l2` regularization\n",
    "- In addition to two steps above, now include `batch_normalization`\n",
    "- At each step visualize how your model is responding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Add more number of layers and repeat step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Credit_default_prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
