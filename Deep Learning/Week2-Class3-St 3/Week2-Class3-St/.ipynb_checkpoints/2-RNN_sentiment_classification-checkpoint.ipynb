{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Import the relevant packages and dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gxe7ZnfzWgkf",
    "outputId": "af50403d-1ce8-4032-d932-3c5881e86717"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import relevant packages\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Preprocess the text to remove punctuation, normalize all words to lowercase, and remove the stopwords, as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Extract the word-to-integer mapping of all the words that constitute the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding step, we are extracting the frequency of all the words in the dataset. A sample of extracted words are as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding code, we are looping through all the words and are assigning an index for each word. A sample of integer to word dictionary is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** Map each word in a given sentence to the corresponding word associated with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.** Extract the maximum length of a sentence and normalize all sentences to the same length by padding them. In the following code, we are looping through all the reviews and storing the length corresponding to each review. Additionally, we are also calculating the maximum length of a review (tweet text):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should note that different tweets have different lengths. However, RNN expects the number of time steps for each input to be the same. In the code below, we are padding a mapped review  with a value of 0, if the length of the review is less than the maximum length of all reviews in dataset. This way, all inputs will have the same length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.** Prepare the training and test datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.** Build the RNN architecture and compile the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that embedding takes the total number of distinct words as input, and creates a vector for each word, where `output_dim` represents the number of dimensions in which the word is to be represented. `input_length` represents the number of words in each sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in the RNN layer, if we want to extract the output of each time step, we say the `return_sequences` parameter is `True`. However, in the use case that we are solving now, we extract the output only after reading through all the input words and thus `return_sequences = False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand why there are 401056 parameters to be estimated in the embedding layer. There are a total of 12,532 unique words, and if we consider that there is no word with an index of 0, it results in a total of 12,533 possible words where each is represented in 32 dimensions and hence (12,533 x 32 = 401,056) parameters to be estimated.\n",
    "\n",
    "There is a set of weights that connect the input to the 40 units of RNN. Given that there are 32 inputs at each time step (where the same set of weights is repeated for each time step), a total of 32 x 40 weights is used to connect the input to the hidden layer. This gives an output that is of 1 x 40 in dimension for each input.\n",
    "\n",
    "Additionally,if you consider the weights-connecting the previous time step's hidden layer to the current time step's hidden layer,we have 40 x 40 number of recurrent weights.\n",
    "\n",
    "Along with weights, we would also have 40 bias terms associated with each of the 40 output and thus a total of (32 x 40 + 40 x 40 + 40 = 2,920) weights.\n",
    "\n",
    "There are a total of 82 weights in the final layer, as the 40 units of the final time step are connected to the two possible output, resulting 40 x 2 weights and 2 biases, and thus a total of 82 units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.** Fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RNN_and_LSTM_sentiment_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
